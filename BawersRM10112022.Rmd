---
title: "BawersRM10112022"
output: 
  html_document:
    highlight: tango
    theme: darkly
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
date: "2022-10-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Preparation
## Let's import and clean the data

```{r}
# Import dataset
tele <- read.csv("tele.csv", stringsAsFactors = T)
str(tele)

# Remove X as it's the unique identifier column
tele$X <- NULL

# Delete the duration variable since it comes after the call takes
# place and thus should not be included in our prediction model
tele$duration <- NULL

# Convert pdays into a dummy variable where 0 represents that
# someone was not called before and 1 represents the opposite
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)

# Remove the pdays column
tele$pdays <- NULL
```

## Preparing the data for analysis

```{r}
# Convert all factors into numerical values for ANN
tele_modelmatrix <- as.data.frame(model.matrix(~. -1, tele))
str(tele_modelmatrix)

# Randomize the rows to prevent sequential data biases (e.g., people are less likely to buy because our employees are tired at the end of the day)
set.seed(12345)
tele_random <- tele_modelmatrix[sample(nrow(tele_modelmatrix)),]

# Create a normalization function for ANN
normalize <- function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

# Apply the normalization function for ANN
tele_norm <- as.data.frame(lapply(tele_random, normalize))
tele_norm
```

# K-means Clustering
## Prepare for K-means clustering
```{r, cache = TRUE}
# Set the seed to save our results
set.seed(123)

# Create a new dataframe based on normalized data
tele_norm_cluster <- tele_norm

# Remove the call outcome because it is the response variable
tele_norm_cluster$yyes <- NULL
str(tele_norm_cluster)
```

## Determine the approrpiate number of clusters
```{r}
# Create a function to set up k-means models with different k
kmeans.wss.k <- function(tele_norm_cluster, k){
  km = kmeans(tele_norm_cluster, k)
  return (km$tot.withinss)
}

# Call the function with k = 4, k = 5, k = 6
kmeans.wss.k(tele_norm_cluster, 4)
kmeans.wss.k(tele_norm_cluster, 5)
kmeans.wss.k(tele_norm_cluster, 6)

# Plot distortion at different k values (up to 30)
kmeans.dis <- function(data_z, maxk){
  dis=(nrow(data_z)-1)*sum(apply(data_z,2,var))
  dis[2:maxk]=sapply (2:maxk, kmeans.wss.k, tele_norm_cluster=tele_norm_cluster)
  return(dis) }
maxk = 30
dis = kmeans.dis(tele_norm_cluster, maxk)
plot(1:maxk, dis, type = 'b', xlab = "Number of Clusters", ylab = "Distortion", col = "black")
```
There seems to be an inflection point at k = 5, so we will use this number of clusters.

## Cluster the data
```{r, cache = TRUE}
# Set the seed to save our results
set.seed(357)

# Apply the k-means algorithm with 5 clusters
client_clusters <- kmeans(tele_norm_cluster, 5)

# Check cluster sizes
client_clusters$size
# Thankfully, there's not a cluster that's overtly large or small.

# Add the response variable back to the dataset to calculate success rates
tele_norm_cluster$y <- tele_norm$yyes

# Add a column with clusters for each individual call and factorize it
tele_norm_cluster$cluster <- client_clusters$cluster
tele_norm_cluster$cluster <- as.factor(tele_norm_cluster$cluster)

# Calculate success rates by cluster
success_by_segm <- tapply(tele_norm_cluster$y, tele_norm_cluster$cluster, mean, na.rm = T)
success_by_segm
``` 
> Through k-means clustering, we identified 5 clusters. The clusters had different call success rates. There were two clusters whose success rates were above the 16.7% threshold determined based on our per-call contribution margin. The other three clusters had significantly lower success rates. Therefore, we are going to apply our machine learning models to these clusters in order to optimize our targeting and, hopefully, achieve a higher success rate.

# ANN
## Preparing datasets for modeling
```{r}
# Create dataframes with the three clusters using conditional filtering
cluster1 = tele_norm_cluster[tele_norm_cluster$cluster == 1,]
cluster3 = tele_norm_cluster[tele_norm_cluster$cluster == 3,]
cluster5 = tele_norm_cluster[tele_norm_cluster$cluster == 5,]
```

## Let's create train and test datasets for each cluster
```{r}
# Do an approximate 80-20 train-test split
cluster1$cluster <- NULL
tele_train1 <- cluster1[1:8628,]
tele_test1 <- cluster1[8629:10785,]

cluster3$cluster <- NULL
tele_train3 <- cluster3[1:7792,]
tele_test3 <- cluster3[7793:9741,]

cluster5$cluster <- NULL
tele_train5 <- cluster5[1:2149,]
tele_test5 <- cluster5[2150:2687,]
```

## Building our initial model

```{r, cache = TRUE}
library(neuralnet)
tele_model1 <- neuralnet(y ~ ., data = tele_train1, hidden = 1)
plot(tele_model1)

tele_model3 <- neuralnet(y ~ ., data = tele_train3, hidden = 1)
plot(tele_model3)

tele_model5 <- neuralnet(y ~ ., data = tele_train5, hidden = 1)
plot(tele_model5)

# What we see already from the plot is that every feature has one input node, that there is one hidden layer with a bias term and one output node with a bias term (you won't be able to see this for some reason... - just take our word for it!)
```

### Reordering the Columns

Before we do anything with our model_results, for convenience, let's put *yyes* as the **first column** in our datasets.
```{r, eval = FALSE}

library(tidyverse)

tele_train %>% 
  relocate(yyes)

tele_test %>%
  relocate(yyes)

```

## Initial Results from our ANN Model
```{r}
# First, we must load the neuralnet package.

library(neuralnet)

modelresults <- neuralnet::compute(tele_model, tele_test[2:54])

predictedtele <- modelresults$net.result
summary(predictedtele)
# This seems to be all correct thus far... now, let's actually create a binary value.

yyes_prediction <- ifelse(predictedtele < 0.5, 0, 1)

# Now, we must make a CrossTable.

library(gmodels)
library(caret)

CrossTable(tele_test$yyes, yyes_prediction, prop.chisq = FALSE)

# Let's also create a confusion matrix.
confusionMatrix(data = as.factor(yyes_prediction), reference = as.factor(tele_test$yyes), positive = "1")
```

## Interpreting Our Initial ANN

> In our ANN, we see that...

## Improving Our ANN Model

```{r}
# Let's do everything again, but now adding some additional layers to the dataset.

```

# Logistic Regression

We have thankfully already cleaned the data up now. Additionally, Logistic Regression forces dummy variables and doesn't need normalized values. That means that we can just randomize two exact sets based on the cleaned "tele" dataset. Let's do that!

```{r}
set.seed(12345)
tele_random_log <- tele[sample(nrow(tele)),]

# Let's create and train and test set.
tele_train_log <- tele_random_log[1:8238,]
tele_test_log <- tele_random_log[8239:41188,]

# In the meantime, let's store our test labels for the test set somewhere else...
tele_test_log_labels <- tele_test_log$y
tele_test_log$y <- NULL

predictionmodel <- glm(y ~ ., data = tele_train_log, family = "binomial")
summary(predictionmodel)
```

### Addressing Multicollinearity (cor() and alias())

First, we need to know why we get the "1 not defined because of singularities" and "rank-deficient fit" pop ups. This basically shows that two variables are perfectly multicollinear. Since NOT all of our variables are numeric, we cannot run the correlation function as done below since we would also have to do this for the factors.

```{r}
# Let's first look at the numerics, though...
cor(tele_train_log[sapply(tele_train_log,is.numeric)])
# We see quite some high correlations; e.g., emp.var.rate and euribor3m have 0.97, but none of them are perfectly correlated...
# So let's do that again with the factors. One quick way is to use alias().

alias(predictionmodel)
# See how loanunknown has a 1 for housingunknown? This means that loanunknown = 1 only when housingunknown = 1, and thus we have a perfectly multicollinear variable. We'll make sure to fix this in our revised model.

```

### Fixing Our Prediction Issue

```{r, error = TRUE}
binarypredictionlog <- ifelse(predict(predictionmodel, newdata = tele_test_log, type = "response") < 0.5, 0, 1)
## Error: factor default has new levels yes. This issue arises because our train_log default column didn't have the value "yes", but our tele_test_log did...
# This is, unfortunately, an issue due to the data structure: "yes" just doesn't happen a lot for "default", and to make matters worse it only occurs in our tele_test_log dataset...

# !!!!!!!!!!! Technically, we could just remove the default column in both our regression and thus our predict, since we can already see that it's not significant. Let's do that! Alternatively, we could deliberately include an observation with default = yes in our train set, but that would skew our comparison relative to ANN and KNN. Thus, doing this is ill-advised.

predictionmodelnew <- glm(y ~ age + job + marital + education + 
                            housing + loan + contact + month + 
                            day_of_week + campaign + previous + 
                            poutcome + emp.var.rate + cons.price.idx +
                            cons.conf.idx + euribor3m + nr.employed +
                            pdaysdummy, data = tele_train_log, family = "binomial")
summary(predictionmodelnew)

binarypredictionlog <- ifelse(predict(predictionmodelnew, newdata = tele_test_log, type = "response") < 0.5, 0, 1)

# Great! Everything's all good now. Next, let's make a crosstable and confusion matrix.
CrossTable(tele_test_log_labels, binarypredictionlog, prop.chisq = FALSE)

##!!!!!!!!!!!!!1 Is this really the solution? !!!!!!!!!!!!!!!!!!!!!!
tele_test_log_labels_y <- ifelse(tele_test_log_labels == "yes", 1, 0)

confusionMatrix(data = as.factor(binarypredictionlog), reference = as.factor(tele_test_log_labels_y), positive = "1")
```

## Improving our Logistic Regression

We already saw that a lot of our variables in the initial glm were insignificant. To have a more robust glm, we must remove some of these variables, which is done below.

```{r}
# Let's first get rid of ALL variables with a p-value exceeding 0.10; thankfully, this also means that we get rid of "default" and the somewhat annoying multicollinearity between housing and loan [unknown].

predictionmodel2 <- glm(y ~ age + job + contact + month + day_of_week + campaign + poutcome + emp.var.rate + cons.price.idx + cons.conf.idx, data = tele_train_log, family = "binomial")

summary(predictionmodel2)

binarypredictionlog2 <- ifelse(predict(predictionmodel2, newdata = tele_test_log, type = "response") < 0.5, 0, 1)

CrossTable(tele_test_log_labels, binarypredictionlog2, prop.chisq = FALSE)

confusionMatrix(data = as.factor(binarypredictionlog2), reference = as.factor(tele_test_log_labels_y), positive = "1")

```

## Interpreting our Logistic Regression 

> Text here.

# KNN

```{r}
library(class)
# We know that for KNN, we have to use a train and test set too, but here we must exclude the response variable.
# So, all we do is create two label variables instead and drop the response variable from the train and test set.

tele_train_labels <- tele_train$yyes
tele_test_labels <- tele_test$yyes

tele_train$yyes <- NULL
tele_test$yyes <- NULL

# Let's now set k
k <- round(sqrt(nrow(tele_train)), 0)
# So, k is 91. Thankfully, this is an odd integer so we're all good to continue with this k.

tele_test_pred_knn <- knn(train = tele_train, test = tele_test, cl = tele_train_labels)

CrossTable(tele_test_labels, tele_test_pred_knn, prop.chisq = F)

confusionMatrix(data = tele_test_pred_knn, reference = as.factor(tele_test_labels), positive = "1")

```

## Interpreting the kNN

> Text here... 


## Improving Our kNN Model

```{r}

```

# A Combined Prediction Model

To start building a combined prediction model, we need a new dataframe with all output values (0,1). 
```{r}

combinedoutputs <- data.frame(yyes_prediction, binarypredictionlog, tele_test_pred_knn)
colnames(combinedoutputs) <- c("ANN", "LogReg", "kNN")

str(combinedoutputs)
# We see that our kNN column is a factor. Let's change that into a numeric for our ifelse later on.

combinedoutputs$kNN <- ifelse(combinedoutputs$kNN == "0", 0, 1)

# Let's have a look again...
str(combinedoutputs)
head(combinedoutputs)

# Everything's all good! Let's now have a real combined output.

combinedoutputs$mix <- ifelse(combinedoutputs$ANN + 
                                combinedoutputs$LogReg + 
                                combinedoutputs$kNN > 1, 1, 0)

# Great! Now let's make a Cross Table and Confusion matrix again.

# Store mix results into a numeric.
combinedresults <- as.numeric(combinedoutputs$mix)

# Create a cross table
CrossTable(tele_test_labels, combinedresults, prop.chisq = F)

# Create a confusion matrix

confusionMatrix(data = as.factor(combinedresults), reference = as.factor(tele_test_labels), positive = "1")
```

## Improving our Combined Prediction Model - An Interesting Task

> NOTE TO US: Let's just explain here that the way to improve a combined model is by improving its roots (being the ANN, LR, and kNN). That said, if we were allowed to add weights to the sum, things would be interesting. We'd add a higher weight to the most accurate model and similarly penalize less accurate models. 

## Interpreting our Combined Prediction Model

> Text here. 



## Applying K-means Clustering to the Test Set

```{r}

```


## Improving our K-means Clustering Model 

```{r}

```

## Interpreting our K-means Clustering Model

> Text here. 

# Conclusion and Recommendations

> Text here....