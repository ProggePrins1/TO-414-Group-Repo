---
title: "BawersRM10112022"
output: 
  html_document:
    highlight: tango
    theme: darkly
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
date: "2022-10-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Preparation
## Let's import and clean the data

```{r}
# Import dataset
tele <- read.csv("tele.csv", stringsAsFactors = T)
str(tele)

# Remove X as it's the unique identifier column
tele$X <- NULL

# Delete the duration variable since it comes after the call takes
# place and thus should not be included in our prediction model
tele$duration <- NULL

# Convert pdays into a dummy variable where 0 represents that
# someone was not called before and 1 represents the opposite
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)

# Remove the pdays column
tele$pdays <- NULL
```

## Preparing the data for analysis

```{r}
# Convert all factors into numerical values for ANN
tele_modelmatrix <- as.data.frame(model.matrix(~. -1, tele))
str(tele_modelmatrix)

# Randomize the rows to prevent sequential data biases (e.g., people are less likely to buy because our employees are tired at the end of the day)
set.seed(12345)
tele_random <- tele_modelmatrix[sample(nrow(tele_modelmatrix)),]

# Create a normalization function for ANN
normalize <- function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

# Apply the normalization function for ANN
tele_norm <- as.data.frame(lapply(tele_random, normalize))
str(tele_norm)
# Looks all good!
```

# K-means Clustering
## Preparing for K-means Clustering
```{r, cache = TRUE}
# Because we will have to delete the yes/no column for our cluster model, it's best to create a new dataset.
# With that, let's create a new dataframe based on normalized data
library(tidyverse)
tele_norm_cluster <- tele_norm %>% select(-yyes)

```

## Determining the Approrpiate Number of Clusters
```{r, cache = TRUE}
# Again, let's set a seed so that we have consistent results
set.seed(357)

# Create a function to set up k-means models with different k
kmeans.wss.k <- function(tele_norm_cluster, k){
  km = kmeans(tele_norm_cluster, k)
  return (km$tot.withinss)
}

# Call the function with k = 4, k = 5, k = 6
kmeans.wss.k(tele_norm_cluster, 3)
kmeans.wss.k(tele_norm_cluster, 4)
kmeans.wss.k(tele_norm_cluster, 5)
kmeans.wss.k(tele_norm_cluster, 6)
# We can see that, of course, the distance decreases as we increase k.
# The gaps aren't too large, so we note that the elbow point must be somewhere around these points (note :
# it's highly likely to be for k = 5, as delta(6,5) = approx. 1000).

# However, to visualize this, we can best plot our distortions.
# Plot distortion at different k values (up to 30)
kmeans.dis <- function(data_z, maxk){
  dis=(nrow(data_z)-1)*sum(apply(data_z,2,var))
  dis[2:maxk]=sapply (2:maxk, kmeans.wss.k, tele_norm_cluster=tele_norm_cluster)
  return(dis) }
maxk = 30
dis = kmeans.dis(tele_norm_cluster, maxk)
plot(1:maxk, dis, type = 'b', xlab = "Number of Clusters", ylab = "Distortion", col = "black")
# Again, cluster 5 appears to be the elbow point, whilst also being acceptable in terms of practicability 
# (e.g., running 30 regression models would be highly impractical and not as insightful/useful from a business perspective)
```

> There seems to be an inflection point at k = 5, so we will use this number of clusters.

## Clustering the data
```{r, cache = TRUE}
# Set another seed to save our cluster results
set.seed(123)

# Apply the k-means algorithm with 5 clusters
client_clusters <- kmeans(tele_norm_cluster, 5)

# Check cluster sizes
client_clusters$size
# Thankfully, there's not a cluster that's overly large or small - this is a good sign! Too large would indicate that we could have used another cluster, whereas too small indicates too much specificity.

# Add the response variable back to the dataset to calculate success rates
tele_norm_cluster$y <- tele_norm$yyes

# Add a column with clusters for each individual call and factorize it
tele_norm_cluster$cluster <- client_clusters$cluster
tele_norm_cluster$cluster <- as.factor(tele_norm_cluster$cluster)

# Calculate success rates by cluster
success_by_segm <- tapply(tele_norm_cluster$y, tele_norm_cluster$cluster, mean, na.rm = T)
success_by_segm
``` 
> Through k-means clustering, we identified 5 clusters. The clusters had different call success rates. There were two clusters whose success rates were above the 16.7% threshold determined based on our per-call contribution margin. The other three clusters had significantly lower success rates. Therefore, we are going to apply our machine learning models to these clusters in order to optimize our targeting and, hopefully, achieve a higher success rate.

# ANN
## Preparing datasets for modeling
```{r}
# Create dataframes with the three clusters using conditional filtering
normcluster2 <- tele_norm_cluster[tele_norm_cluster$cluster == 2,] %>% select(-cluster)
normcluster4 <- tele_norm_cluster[tele_norm_cluster$cluster == 4,] %>% select(-cluster)
normcluster5 <- tele_norm_cluster[tele_norm_cluster$cluster == 5,] %>% select(-cluster)
```

## Let's create train and test datasets for each cluster
```{r}
# Do an approximate 80-20 train-test split. We prevented hard-coding this by using the floor() function.

tele_train2 <- normcluster2[1:floor(0.8*nrow(normcluster2)),]
tele_test2 <- normcluster2[(floor(0.8*nrow(normcluster2)+1)):nrow(normcluster2),]

tele_train4 <- normcluster4[1:floor(0.8*nrow(normcluster4)),]
tele_test4 <- normcluster4[(floor(0.8*nrow(normcluster4)+1)):nrow(normcluster4),]

tele_train5 <- normcluster5[1:floor(0.8*nrow(normcluster5)),]
tele_test5 <- normcluster5[(floor(0.8*nrow(normcluster5)+1)):nrow(normcluster5),]
```

## Building our initial model

```{r, cache = TRUE}
library(neuralnet)
tele_model2 <- neuralnet(y ~., data = tele_train2, hidden = 1)
plot(tele_model2)

tele_model4 <- neuralnet(y ~ ., data = tele_train4, hidden = 1, stepmax = 1e+08)
plot(tele_model4)

tele_model5 <- neuralnet(y ~ ., data = tele_train5, hidden = 1, stepmax = 1e+08)
plot(tele_model5)

# What we see already from the plot is that every feature has one input node, that there is one hidden layer with a bias term and one output node with a bias term (you won't be able to see this for some reason... - just take our word for it!)
```

## Initial Results from our ANN Model
```{r}
# First, we must load the neuralnet package.

library(neuralnet)

# Let's exclude the y column in our tele_test. We don't want to run ANN with that included.
modelresults2 <- compute(tele_model2, tele_test2[1:53])
modelresults4 <- compute(tele_model4, tele_test4[1:53])
modelresults5 <- compute(tele_model5, tele_test5[1:53])

predictedtele2 <- modelresults2$net.result
summary(predictedtele2)
# We already see that the values are incredibly low. This is because the success rates of this cluster are incredibly low.
# Note that this isn't bad, per se - it just tells us that we shouldn't call this cluster.
predictedtele4 <- modelresults4$net.result
summary(predictedtele4)
# See interpretation for predictedtele2 - the same applies here.
predictedtele5 <- modelresults5$net.result
summary(predictedtele5)
# This seems to be all correct thus far... now, let's actually create a binary value.

annprediction2 <- ifelse(predictedtele2 < 0.5, 0, 1)
annprediction4 <- ifelse(predictedtele4 < 0.5, 0, 1)
annprediction5 <- ifelse(predictedtele5 < 0.5, 0, 1)

# Now, we must make a Cross Table and Confusion Matrix.

library(gmodels)
library(caret)

# For cluster = 2
CrossTable(tele_test2$y, annprediction2, prop.chisq = FALSE)
confusionMatrix(data = as.factor(annprediction2), reference = as.factor(tele_test2$y), positive = "1")

# For cluster = 4
CrossTable(tele_test4$y, annprediction4, prop.chisq = FALSE)
confusionMatrix(data = as.factor(annprediction4), reference = as.factor(tele_test4$y), positive = "1")

# For cluster = 5
CrossTable(tele_test5$y, annprediction5, prop.chisq = FALSE)
confusionMatrix(data = as.factor(annprediction5), reference = as.factor(tele_test5$y), positive = "1")
```

## Interpreting Our Initial ANN

This interpretation section - and any following interpretation sections - have been separated **per cluster**. As a disclaimer, *we did try to tinker with the cut-off value of 0.5, but this either (1) only made the model less accurate because the trained model is mainly based on unsuccessful calls, or (2) increased the accuracy only marginally. In the latter case, we carried this forward but since this already counts towards "improving the model", we'll only incorporate this later on*. Instead, we'll try to improve the model by adding hidden layers later on. 

### Cluster 2

With a cut-off point at 0.5, our ANN model never predicts that someone will say yes. Although this may seem weird at first, the model is trained with over 94% of people actually saying no, so it does not learn that well what differentiates a yes from a no. That said, it predicted that *all 1953 people would say no*, being correct 94.47% of the time. The false negatives are worrying, though. We could have gotten 108 "yes'" if our ANN model were a bit stronger. That said, with such a model, we would prevent calling 1845 people that wouldn't have given us the yes-word anyways [kappa = 0 since detection rate = 0]. This proves that such a strategy significantly outperforms the idea of calling every single person.

### Cluster 4

With a cut-off point at 0.5, our ANN model predicts that someone will say no 1234 times, whilst only being correct 1180 times (95.6%), whereas it predicts a yes 8 times, whilst only being correct 3 times (37.5%). This shows clear room of improvement as we now have 54 false negatives, leading to lost sales, whilst also incurring some costs by calling people who wouldn't want our offering. Again, however, such a model again shows that we shouldn't call everyone [kappa = 0.08].

### Cluster 5

With a cut-off point at 0.5, our ANN model predicts that someone will say no 1854 times, whilst only being correct 1773 times (95.6%), whereas it predicts a yes 1 time, which is a correct prediction. Still, the 81 false negatives are worrying as this shows significant lost revenues. Thankfully, we wouldn't incur unnecessary costs by calling someone who wouldn't say yes anyways! Again, however, such a model again shows that we shouldn't call everyone [kappa = 0.02].

## Improving Our ANN Model

```{r}
# Let's do everything again, but now adding some additional layers to the ANN cluster models.

# Choosing the hidden levels is somewhat arbitrary. To not overload our computers, we went for hidden = 2.

improvedtele_model2 <- neuralnet(y ~., data = tele_train2, hidden = 2)
plot(tele_model2)

improvedtele_model4 <- neuralnet(y ~ ., data = tele_train4, hidden = 2, stepmax = 1e+08)
plot(tele_model4)

improvedtele_model5 <- neuralnet(y ~ ., data = tele_train5, hidden = 2, stepmax = 1e+08)
plot(tele_model5)

```

## Results from our Improved ANN Model

```{r}
# Let's exclude the y column in our tele_test. We don't want to run ANN with that included.
improvedmodelresults2 <- compute(improvedtele_model2, tele_test2[1:53])
improvedmodelresults4 <- compute(improvedtele_model4, tele_test4[1:53])
improvedmodelresults5 <- compute(improvedtele_model5, tele_test5[1:53])

improvedpredictedtele2 <- improvedmodelresults2$net.result
summary(improvedpredictedtele2)
# We already see that the values stay incredibly low. Again, this is because the success rates of this cluster are incredibly low.
# Note, again, that this isn't bad, per se - it just tells us that we shouldn't call this cluster.
improvedpredictedtele4 <- improvedmodelresults4$net.result
summary(improvedpredictedtele4)
# See interpretation for predictedtele2 - the same applies here.
improvedpredictedtele5 <- improvedmodelresults5$net.result
summary(improvedpredictedtele5)
# This seems to be all correct thus far... now, let's actually create a binary value.

improvedannprediction2 <- ifelse(improvedpredictedtele2 < 0.5, 0, 1)
improvedannprediction4 <- ifelse(improvedpredictedtele4 < 0.22, 0, 1)
improvedannprediction5 <- ifelse(improvedpredictedtele5 < 0.18, 0, 1)

# Now, let's create a Cross Table and Confusion Matrix.

# For cluster = 2
CrossTable(tele_test2$y, improvedannprediction2, prop.chisq = FALSE)
confusionMatrix(data = as.factor(improvedannprediction2), reference = as.factor(tele_test2$y), positive = "1")

# For cluster = 4
CrossTable(tele_test4$y, improvedannprediction4, prop.chisq = FALSE)
confusionMatrix(data = as.factor(improvedannprediction4), reference = as.factor(tele_test4$y), positive = "1")

# For cluster = 5
CrossTable(tele_test5$y, improvedannprediction5, prop.chisq = FALSE)
confusionMatrix(data = as.factor(improvedannprediction5), reference = as.factor(tele_test5$y), positive = "1")

```

## Interpreting our Improved ANN Model

> For this section, we *DID* adjust the cut-off points using a trial-and-error method to ensure that the accuracy estimates are as high as possible. As a rule of thumb, we tried to prevent as many false negatives as possible whilst preventing that false positives would grow significantly (line of thought: $6 in lost sales > $1 in costs, but not worth it if one decrease in FN comes paired with 6 or more increases in FP).

### Cluster 2

For cluster 2, it's still best to have a cut-off point of 0.5, purely because it's simply not worth it to call anyone in this segment given the ANN model. Lowering the cut-off point would result in a lot of costs from calling people who wouldn't say yes to our offering. As such, we'd incur losses of 108 * - 6 = -€648. That said, if we were to call everyone, we'd have profits of  108 * 6 - 1953 * 1 = -€1305. So, our incremental profits would be 1305 - 648 = €657 (although we'd still make losses). 

### Cluster 4

For cluster 4, it's best to have a cut-off point of 0.22, again because our ANN has a very low prediction value given the low success rate of the fourth cluster AND because reducing false negatives (-6) is more important than lowering false positives (-1). The profits in this case would be 3 * 6 - 54 * 6 - 3 * 1 = -€309, so unfortunately still negative. Incremental profits, however, would be positive. If we were to call everyone, we'd have profits of 57 * 6 - 1242 * 1 = -€900, making incremental profits of 900 - 309 = €591 by implementing an ANN model.

### Cluster 5

For cluster 5, it's best to have a cut-off point of 0.18, as our ANN has a very low prediction value of 0.03 for even observations that actually said "yes" in real life to our call. The total costs in this case would be 81 * 6 = 486 in lost sales and 8 * 1 = 8 in costs, resulting in a total of €494. Profits would be €6 * 1 - €494 = -€488  Still not incredible, but the main finding is that cluster 5 might not be worth it to call (oh, and don't forget to compare this to calling all people in the test set of cluster 5, with 1855 * 1 = €1855, and revenue of 82 * €6 = €492. Profits = €492 - €1855 = -€1363). So, based on the status quo, we'd still gain 1363 - 488 = €875.

### Final Conclusion

> Simply put, ANN works. We cannot make these low-success clusters into money machines, and the model is not perfect in preventing us from calling anyone who will say no, but we have saved costs tremendously by using ANN. Savings are 657 + 591 + 875 = €2123.




# Logistic Regression

We have thankfully already cleaned the data up now. Additionally, Logistic Regression forces dummy variables and doesn't need normalized values. So, we have to make sure that we create cluster sets that are *not normalized*.

## Setting up our data
```{r}
# Thankfully, we set a seed before, so we can still create a new sample without messing up the order of rows and clusters.
set.seed(12345)
tele_random_log <- tele[sample(nrow(tele)),]

# The next step is to add a cluster column to our random log dataset.
tele_random_log$cluster <- client_clusters$cluster
head(tele_random_log)
# And yes, that looks all good! 

# Now, let's create train and test sets again for clusters 2, 4, and 5. 
# We'll stick to the 80%/20% rule to maintain consistency in our comparisons. 

cluster2 <- tele_random_log[tele_random_log$cluster == 2,] %>% select (-cluster) 
cluster4 <- tele_random_log[tele_random_log$cluster == 4,] %>% select (-cluster)
cluster5 <- tele_random_log[tele_random_log$cluster == 5,] %>% select (-cluster)

tele_train2_log <- cluster2[1:nrow(tele_train2),]
tele_test2_log <- cluster2[(nrow(tele_train2)+1):nrow(cluster2),]

tele_train4_log <- cluster4[1:nrow(tele_train4),]
tele_test4_log <- cluster4[(nrow(tele_train4)+1):nrow(cluster4),]

tele_train5_log <- cluster5[1:nrow(tele_train5),]
tele_test5_log <- cluster5[(nrow(tele_train5)+1):nrow(cluster5),]

# In the meantime, let's store our test labels for the test set somewhere else..
tele_test2_log_labels <- tele_test2_log$y
tele_test4_log_labels <- tele_test4_log$y
tele_test5_log_labels <- tele_test5_log$y

# Of course, we now must remove the y from our test set.
tele_test2_log$y <- NULL
tele_test4_log$y <- NULL
tele_test5_log$y <- NULL

```

## Running the initial prediction models 
```{r}
predictionmodelt2 <- glm(y ~ ., data = tele_train2_log, family = "binomial")
summary(predictionmodelt2)

predictionmodelt4 <- glm(y ~ ., data = tele_train4_log, family = "binomial")
summary(predictionmodelt4)

predictionmodelt5 <- glm(y ~ ., data = tele_train5_log, family = "binomial")
summary(predictionmodelt5)

# Unfortunately, we get an error here. It appears to be the case that for our tele_train5_log, we have a predictor variable which only has one level, causing the regression to fail. Let's find and fix this issue.
# To find the column with only one factorized value, we can use lapply() and unique as an argument.

lapply(tele_train5_log, unique)
# We see that the factor contact only has the value telephone. Let's confirm that.
sum(tele_train5_log$contact == "cellular")
sum(tele_train5_log$contact == "telephone")
# So, the only solution here is to remove the contact variable from the train set.
tele_train5_log$contact <- NULL
# Note that pdaysdummy only contains the value 0 and is thus pointless in our glm. Let's remove that column, too.
tele_train5_log$pdaysdummy <- NULL

predictionmodelt5 <- glm(y ~ ., tele_train5_log, family = "binomial")
# now we get two more warnings, so let's fix those too.
# first, we see that two predictors are perfectly correlated. By merely looking at the dataset, we denote that this must be between previous and poutcome. Everyone who was called previously had a failure. Since multicollinearity can hurt our model, we must remove one of the variables.

# Let's confirm this first
cor(as.numeric(tele_train5_log$y), as.numeric(tele_train5_log$poutcome))
cor(as.numeric(tele_train5_log$y), tele_train5_log$previous)
# Yes, we must remove one variable. Let's choose to drop previous.

tele_train5_log$previous <- NULL

predictionmodelt5 <- glm(y ~ ., tele_train5_log, family = "binomial")

summary(predictionmodelt5)
```

> The thing is that our models are nowhere near strong. We still have tons of variables that will likely be highly multicollinear, and we actually must remove variables before we can run predictions. Below is explained why.

## Using Linear Regression on the Test Data - why it doesn't "just" work.

```{r}
binarypredictionlogt2 <- ifelse(predict(predictionmodelt2, newdata = tele_test2_log, type = "response") < 0.5, 0, 1)

binarypredictionlogt4 <- ifelse(predict(predictionmodelt4, newdata = tele_test4_log, type = "response") < 0.5, 0, 1)

binarypredictionlogt5 <- ifelse(predict(predictionmodelt5, newdata = tele_test5_log, type = "response") < 0.5, 0, 1)

## In all prediction models, we have the issue that our test dataset has some values for "month" that aren't in the train set. R now has no clue how, for instance, October impacts the y column, and thus cannot run predictions.
```

Technically, we could just remove the month column in both our regression and thus our predict, since we can already see that it's not significant. Let's do that! Alternatively, we could deliberately include an observation with month = the month needed in our train set, but that would skew our comparison relative to ANN and KNN. Thus, doing this is ill-advised.

## Removing Columns in Linear Regression

```{r}

# CLUSTER 2!!!!!!

improvedpredictionmodelt2 <- glm(y ~ job + marital + month, data = tele_train2_log, family = "binomial")
summary(improvedpredictionmodelt2)

# CLUSTER 4!!!!!!
# Let's remove the month column.
# We first realized the singularities warning message. This means that one variable is perfectly correlated to the other.
# we can use alias() to find which ones.

alias(predictionmodelt4)
# Okay, so pdaysdummy is perfectly correlated to one value of poutcome. Let's drop pdaysdummy too, then.

# Improvement procedure: sequentially remove variables with p > 0.10. 

improvedpredictionmodelt4 <- glm(y ~ education + contact + day_of_week + poutcome + euribor3m + nr.employed, data = tele_train4_log, family = "binomial")
summary(improvedpredictionmodelt4)

# CLUSTER 5!!!!!!
# Let's remove nr.employed first because it contains NA's.
improvedpredictionmodelt5 <- glm(y ~. - nr.employed, data = tele_train5_log, family = "binomial")
summary(improvedpredictionmodelt5)
# We see that, thankfully, now we have more realistic p-values! We can now use our normal
# procedure to refine the regression.

improvedpredictionmodelt5 <- glm(y ~ job + marital + housing + euribor3m, data = tele_train5_log, family = "binomial")
summary(improvedpredictionmodelt5)

```

## Rerun Prediction Models

```{r}

# Cluster 2

improvedbinarypredictionlogt2 <- ifelse(predict(improvedpredictionmodelt2, newdata = tele_test2_log, type = "response") < 0.4, 0, 1)
# Because it was never explained in the lecture how to solve the issue of a factor value in the test set that was 
# not in the train set, we had to devise our own solution to this issue.
# First, we looked at the number of values that had the month october. 
tele_test2_log[which(tele_test2_log$month == "oct"),]
# It turns out to be only one person, who would have said NO.
# Now we can opt for one practical solution, although it's not "pretty". 
# We have to remove this row from our test set and our labels set. Unfortunately, there is no other option.
# We did not have to do this for the other regression models because the months were not significant to start with.
# Yet, in cluster 2, the monthjul is a significant variable.

which(tele_test2_log$month == "oct", arr.ind = T)
# we now know it's row 1723. So, let's remove that row.
tele_test2_log <- tele_test2_log[-1723,]

# To remove a row, change labels into a df.
tele_test2_log_labels <- as.data.frame(tele_test2_log_labels)

# Let's remove the 1723rd row in our df.
tele_test2_log_labels <- tele_test2_log_labels[-1723,]

CrossTable(tele_test2_log_labels, improvedbinarypredictionlogt2, prop.chisq = FALSE)

improvedbinarypredictionlogt2 <- ifelse(predict(improvedpredictionmodelt2, newdata = tele_test2_log, type = "response") < 0.4, 0, 1)
CrossTable(tele_test2_log_labels, improvedbinarypredictionlogt2, prop.chisq = FALSE)

# Cluster 4

improvedbinarypredictionlogt4 <- ifelse(predict(improvedpredictionmodelt4, newdata = tele_test4_log, type = "response") < 0.4, 0, 1)
CrossTable(tele_test4_log_labels, improvedbinarypredictionlogt4, prop.chisq = FALSE)

# Cluster 5

improvedbinarypredictionlogt5 <- ifelse(predict(improvedpredictionmodelt5, newdata = tele_test5_log, type = "response") < 0.4, 0, 1)
CrossTable(tele_test5_log_labels, improvedbinarypredictionlogt5, prop.chisq = FALSE)

```


### Addressing Multicollinearity (cor() and alias())

First, we need to know why we get the "1 not defined because of singularities" and "rank-deficient fit" pop ups. This basically shows that two variables are perfectly multicollinear. Since NOT all of our variables are numeric, we cannot run the correlation function as done below since we would also have to do this for the factors.

```{r}
# Let's first look at the numerics, though...
cor(tele_train_log[sapply(tele_train_log,is.numeric)])
# We see quite some high correlations; e.g., emp.var.rate and euribor3m have 0.97, but none of them are perfectly correlated...


# See how loanunknown has a 1 for housingunknown? This means that loanunknown = 1 only when housingunknown = 1, and thus we have a perfectly multicollinear variable. We'll make sure to fix this in our revised model.

```

### Fixing Our Prediction Issue

```{r, error = TRUE}
binarypredictionlog <- ifelse(predict(predictionmodel, newdata = tele_test_log, type = "response") < 0.5, 0, 1)
## Error: factor default has new levels yes. This issue arises because our train_log default column didn't have the value "yes", but our tele_test_log did...
# This is, unfortunately, an issue due to the data structure: "yes" just doesn't happen a lot for "default", and to make matters worse it only occurs in our tele_test_log dataset...

# !!!!!!!!!!! Technically, we could just remove the default column in both our regression and thus our predict, since we can already see that it's not significant. Let's do that! Alternatively, we could deliberately include an observation with default = yes in our train set, but that would skew our comparison relative to ANN and KNN. Thus, doing this is ill-advised.

predictionmodelnew <- glm(y ~ age + job + marital + education + 
                            housing + loan + contact + month + 
                            day_of_week + campaign + previous + 
                            poutcome + emp.var.rate + cons.price.idx +
                            cons.conf.idx + euribor3m + nr.employed +
                            pdaysdummy, data = tele_train_log, family = "binomial")
summary(predictionmodelnew)

binarypredictionlog <- ifelse(predict(predictionmodelnew, newdata = tele_test_log, type = "response") < 0.5, 0, 1)

# Great! Everything's all good now. Next, let's make a crosstable and confusion matrix.
CrossTable(tele_test_log_labels, binarypredictionlog, prop.chisq = FALSE)

##!!!!!!!!!!!!!1 Is this really the solution? !!!!!!!!!!!!!!!!!!!!!!
tele_test_log_labels_y <- ifelse(tele_test_log_labels == "yes", 1, 0)

confusionMatrix(data = as.factor(binarypredictionlog), reference = as.factor(tele_test_log_labels_y), positive = "1")
```

## Improving our Logistic Regression

We already saw that a lot of our variables in the initial glm were insignificant. To have a more robust glm, we must remove some of these variables, which is done below.

```{r}
# Let's first get rid of ALL variables with a p-value exceeding 0.10; thankfully, this also means that we get rid of "default" and the somewhat annoying multicollinearity between housing and loan [unknown].

predictionmodel2 <- glm(y ~ age + job + contact + month + day_of_week + campaign + poutcome + emp.var.rate + cons.price.idx + cons.conf.idx, data = tele_train_log, family = "binomial")

summary(predictionmodel2)

binarypredictionlog2 <- ifelse(predict(predictionmodel2, newdata = tele_test_log, type = "response") < 0.5, 0, 1)

CrossTable(tele_test_log_labels, binarypredictionlog2, prop.chisq = FALSE)

confusionMatrix(data = as.factor(binarypredictionlog2), reference = as.factor(tele_test_log_labels_y), positive = "1")

```

## Interpreting our Logistic Regression 

> Text here.

# KNN

```{r}
library(class)
# We know that for KNN, we have to use a train and test set too, but here we must exclude the response variable.
# So, all we do is create two label variables instead and drop the response variable from the train and test set.

tele_train_labels <- tele_train$yyes
tele_test_labels <- tele_test$yyes

tele_train$yyes <- NULL
tele_test$yyes <- NULL

# Let's now set k
k <- round(sqrt(nrow(tele_train)), 0)
# So, k is 91. Thankfully, this is an odd integer so we're all good to continue with this k.

tele_test_pred_knn <- knn(train = tele_train, test = tele_test, cl = tele_train_labels)

CrossTable(tele_test_labels, tele_test_pred_knn, prop.chisq = F)

confusionMatrix(data = tele_test_pred_knn, reference = as.factor(tele_test_labels), positive = "1")

```

## Interpreting the kNN

> Text here... 


## Improving Our kNN Model

```{r}

```

# A Combined Prediction Model

To start building a combined prediction model, we need a new dataframe with all output values (0,1). 
```{r}

combinedoutputs <- data.frame(yyes_prediction, binarypredictionlog, tele_test_pred_knn)
colnames(combinedoutputs) <- c("ANN", "LogReg", "kNN")

str(combinedoutputs)
# We see that our kNN column is a factor. Let's change that into a numeric for our ifelse later on.

combinedoutputs$kNN <- ifelse(combinedoutputs$kNN == "0", 0, 1)

# Let's have a look again...
str(combinedoutputs)
head(combinedoutputs)

# Everything's all good! Let's now have a real combined output.

combinedoutputs$mix <- ifelse(combinedoutputs$ANN + 
                                combinedoutputs$LogReg + 
                                combinedoutputs$kNN > 1, 1, 0)

# Great! Now let's make a Cross Table and Confusion matrix again.

# Store mix results into a numeric.
combinedresults <- as.numeric(combinedoutputs$mix)

# Create a cross table
CrossTable(tele_test_labels, combinedresults, prop.chisq = F)

# Create a confusion matrix

confusionMatrix(data = as.factor(combinedresults), reference = as.factor(tele_test_labels), positive = "1")
```

## Improving our Combined Prediction Model - An Interesting Task

> NOTE TO US: Let's just explain here that the way to improve a combined model is by improving its roots (being the ANN, LR, and kNN). That said, if we were allowed to add weights to the sum, things would be interesting. We'd add a higher weight to the most accurate model and similarly penalize less accurate models. 

## Interpreting our Combined Prediction Model

> Text here. 



## Applying K-means Clustering to the Test Set

```{r}

```


## Improving our K-means Clustering Model 

```{r}

```

## Interpreting our K-means Clustering Model

> Text here. 

# Conclusion and Recommendations

> Text here....