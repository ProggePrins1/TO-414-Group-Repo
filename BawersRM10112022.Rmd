---
title: "BawersRM10112022"
output: 
  html_document:
    highlight: tango
    theme: darkly
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
date: "2022-10-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Preparation
## Let's import and clean the data

```{r}
# Import dataset
tele <- read.csv("tele.csv", stringsAsFactors = T)
str(tele)

# Remove X as it's the unique identifier column
tele$X <- NULL

# Delete the duration variable since it comes after the call takes
# place and thus should not be included in our prediction model
tele$duration <- NULL

# Convert pdays into a dummy variable where 0 represents that
# someone was not called before and 1 represents the opposite
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)

# Remove the pdays column
tele$pdays <- NULL
```

## Preparing the data for analysis

```{r}
# Convert all factors into numerical values for ANN
tele_modelmatrix <- as.data.frame(model.matrix(~. -1, tele))
str(tele_modelmatrix)

# Randomize the rows to prevent sequential data biases (e.g., people are less likely to buy because our employees are tired at the end of the day)
set.seed(12345)
tele_random <- tele_modelmatrix[sample(nrow(tele_modelmatrix)),]

# Create a normalization function for ANN
normalize <- function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

# Apply the normalization function for ANN
tele_norm <- as.data.frame(lapply(tele_random, normalize))
str(tele_norm)
# Looks all good!
```

# K-means Clustering
## Preparing for K-means Clustering
```{r, cache = TRUE}
# Because we will have to delete the yes/no column for our cluster model, it's best to create a new dataset.
# With that, let's create a new dataframe based on normalized data
library(tidyverse)
tele_norm_cluster <- tele_norm %>% select(-yyes)

```

## Determining the Approrpiate Number of Clusters
```{r, cache = TRUE}
# Again, let's set a seed so that we have consistent results
set.seed(357)

# Create a function to set up k-means models with different k
kmeans.wss.k <- function(tele_norm_cluster, k){
  km = kmeans(tele_norm_cluster, k)
  return (km$tot.withinss)
}

# Call the function with k = 4, k = 5, k = 6
kmeans.wss.k(tele_norm_cluster, 3)
kmeans.wss.k(tele_norm_cluster, 4)
kmeans.wss.k(tele_norm_cluster, 5)
kmeans.wss.k(tele_norm_cluster, 6)
# We can see that, of course, the distance decreases as we increase k.
# The gaps aren't too large, so we note that the elbow point must be somewhere around these points (note :
# it's highly likely to be for k = 5, as delta(6,5) = approx. 1000).

# However, to visualize this, we can best plot our distortions.
# Plot distortion at different k values (up to 30)
kmeans.dis <- function(data_z, maxk){
  dis=(nrow(data_z)-1)*sum(apply(data_z,2,var))
  dis[2:maxk]=sapply (2:maxk, kmeans.wss.k, tele_norm_cluster=tele_norm_cluster)
  return(dis) }
maxk = 30
dis = kmeans.dis(tele_norm_cluster, maxk)
plot(1:maxk, dis, type = 'b', xlab = "Number of Clusters", ylab = "Distortion", col = "black")
# Again, cluster 5 appears to be the elbow point, whilst also being acceptable in terms of practicability 
# (e.g., running 30 regression models would be highly impractical and not as insightful/useful from a business perspective)
```

> There seems to be an inflection point at k = 5, so we will use this number of clusters.

## Clustering the data
```{r, cache = TRUE}
# Set another seed to save our cluster results
set.seed(123)

# Apply the k-means algorithm with 5 clusters
client_clusters <- kmeans(tele_norm_cluster, 5)

# Check cluster sizes
client_clusters$size
# Thankfully, there's not a cluster that's overly large or small - this is a good sign! Too large would indicate that we could have used another cluster, whereas too small indicates too much specificity.

# Add the response variable back to the dataset to calculate success rates
tele_norm_cluster$y <- tele_norm$yyes

# Add a column with clusters for each individual call and factorize it
tele_norm_cluster$cluster <- client_clusters$cluster
tele_norm_cluster$cluster <- as.factor(tele_norm_cluster$cluster)

# Calculate success rates by cluster
success_by_segm <- tapply(tele_norm_cluster$y, tele_norm_cluster$cluster, mean, na.rm = T)
success_by_segm
``` 
> Through k-means clustering, we identified 5 clusters. The clusters had different call success rates. There were two clusters whose success rates were above the 16.7% threshold determined based on our per-call contribution margin. The other three clusters had significantly lower success rates. Therefore, we are going to apply our machine learning models to these clusters in order to optimize our targeting and, hopefully, achieve a higher success rate.





# Logistic Regression

We have thankfully already cleaned the data up now. Additionally, Logistic Regression forces dummy variables and doesn't need normalized values. So, we have to make sure that we create cluster sets that are *not normalized*.

## Setting up our data
```{r}
# Thankfully, we set a seed before, so we can still create a new sample without messing up the order of rows and clusters.
set.seed(12345)
tele_random_log <- tele[sample(nrow(tele)),]

# The next step is to add a cluster column to our random log dataset.
tele_random_log$cluster <- client_clusters$cluster
head(tele_random_log)
# And yes, that looks all good! 

# Now, let's create train and test sets again for clusters 2, 4, and 5. 
# We'll stick to the 80%/20% rule to maintain consistency in our comparisons. 

cluster2 <- tele_random_log[tele_random_log$cluster == 2,] %>% select (-cluster) 
cluster4 <- tele_random_log[tele_random_log$cluster == 4,] %>% select (-cluster)
cluster5 <- tele_random_log[tele_random_log$cluster == 5,] %>% select (-cluster)

tele_train2_log <- cluster2[1:nrow(tele_train2),]
tele_test2_log <- cluster2[(nrow(tele_train2)+1):nrow(cluster2),]

tele_train4_log <- cluster4[1:nrow(tele_train4),]
tele_test4_log <- cluster4[(nrow(tele_train4)+1):nrow(cluster4),]

tele_train5_log <- cluster5[1:nrow(tele_train5),]
tele_test5_log <- cluster5[(nrow(tele_train5)+1):nrow(cluster5),]

# In the meantime, let's store our test labels for the test set somewhere else..
tele_test2_log_labels <- tele_test2_log$y
tele_test4_log_labels <- tele_test4_log$y
tele_test5_log_labels <- tele_test5_log$y

# Of course, we now must remove the y from our test set.
tele_test2_log$y <- NULL
tele_test4_log$y <- NULL
tele_test5_log$y <- NULL

```

## Running the initial prediction models 
```{r}
predictionmodelt2 <- glm(y ~ ., data = tele_train2_log, family = "binomial")
summary(predictionmodelt2)

predictionmodelt4 <- glm(y ~ ., data = tele_train4_log, family = "binomial")
summary(predictionmodelt4)

predictionmodelt5 <- glm(y ~ ., data = tele_train5_log, family = "binomial")
summary(predictionmodelt5)

# Unfortunately, we get an error here. It appears to be the case that for our tele_train5_log, we have a predictor variable which only has one level, causing the regression to fail. Let's find and fix this issue.
# To find the column with only one factorized value, we can use lapply() and unique as an argument.

lapply(tele_train5_log, unique)
# We see that the factor contact only has the value telephone. Let's confirm that.
sum(tele_train5_log$contact == "cellular")
sum(tele_train5_log$contact == "telephone")
# So, the only solution here is to remove the contact variable from the train set.
tele_train5_log$contact <- NULL
# Note that pdaysdummy only contains the value 0 and is thus pointless in our glm. Let's remove that column, too.
tele_train5_log$pdaysdummy <- NULL

predictionmodelt5 <- glm(y ~ ., tele_train5_log, family = "binomial")
# now we get two more warnings, so let's fix those too.
# first, we see that two predictors are perfectly correlated. By merely looking at the dataset, we denote that this must be between previous and poutcome. Everyone who was called previously had a failure. Since multicollinearity can hurt our model, we must remove one of the variables.

# Let's confirm this first
cor(as.numeric(tele_train5_log$y), as.numeric(tele_train5_log$poutcome))
cor(as.numeric(tele_train5_log$y), tele_train5_log$previous)
# Yes, we must remove one variable. Let's choose to drop previous.

tele_train5_log$previous <- NULL

predictionmodelt5 <- glm(y ~ ., tele_train5_log, family = "binomial")

summary(predictionmodelt5)
```

> The thing is that our models are nowhere near strong. We still have tons of variables that will likely be highly multicollinear, and we actually must remove variables before we can run predictions. Below is explained why.

## Using Logistic Regression on the Test Data - why it doesn't "just" work.

```{r}
binarypredictionlogt2 <- ifelse(predict(predictionmodelt2, newdata = tele_test2_log, type = "response") < 0.5, 0, 1)

binarypredictionlogt4 <- ifelse(predict(predictionmodelt4, newdata = tele_test4_log, type = "response") < 0.5, 0, 1)

binarypredictionlogt5 <- ifelse(predict(predictionmodelt5, newdata = tele_test5_log, type = "response") < 0.5, 0, 1)

## In all prediction models, we have the issue that our test dataset has some values for "month" that aren't in the train set. R now has no clue how, for instance, October impacts the y column, and thus cannot run predictions.
```

Technically, we could just remove the month column in both our regression and thus our predict, since we can already see that it's not significant. Let's do that! Alternatively, we could deliberately include an observation with month = the month needed in our train set, but that would skew our comparison relative to ANN and KNN. Thus, doing this is ill-advised.

## Removing Columns in Logistic Regression

```{r}

# CLUSTER 2!!!!!!

improvedpredictionmodelt2 <- glm(y ~ job + marital + month, data = tele_train2_log, family = "binomial")
summary(improvedpredictionmodelt2)

# CLUSTER 4!!!!!!
# Let's remove the month column.
# We first realized the singularities warning message. This means that one variable is perfectly correlated to the other.
# we can use alias() to find which ones.

alias(predictionmodelt4)
# Okay, so pdaysdummy is perfectly correlated to one value of poutcome. Let's drop pdaysdummy too, then.

# Improvement procedure: sequentially remove variables with p > 0.10. 

improvedpredictionmodelt4 <- glm(y ~ education + contact + day_of_week + poutcome + euribor3m + nr.employed, data = tele_train4_log, family = "binomial")
summary(improvedpredictionmodelt4)

# CLUSTER 5!!!!!!
# Let's remove nr.employed first because it contains NA's.
improvedpredictionmodelt5 <- glm(y ~. - nr.employed, data = tele_train5_log, family = "binomial")
summary(improvedpredictionmodelt5)
# We see that, thankfully, now we have more realistic p-values! We can now use our normal procedure to refine the regression.

improvedpredictionmodelt5 <- glm(y ~ job + marital + housing + euribor3m, data = tele_train5_log, family = "binomial")
summary(improvedpredictionmodelt5)

```

## Rerun Improved Prediction Models

```{r}

# Cluster 2

improvedbinarypredictionlogt2 <- ifelse(predict(improvedpredictionmodelt2, newdata = tele_test2_log, type = "response") < 0.4, 0, 1)
# Because it was never explained in the lecture how to solve the issue of a factor value in the test set that was 
# not in the train set, we had to devise our own solution to this issue.
# First, we looked at the number of values that had the month october. 
tele_test2_log[which(tele_test2_log$month == "oct"),]
# It turns out to be only one person, who would have said NO.
# Now we can opt for one practical solution, although it's not "pretty". 
# We have to remove this row from our test set and our labels set. Unfortunately, there is no other option.
# We did not have to do this for the other regression models because the months were not significant to start with.
# Yet, in cluster 2, the monthjul is a significant variable.
# That said, we would also have to remove that row from the labels. This requires a small extra step.

tele_test2_log$y <- tele_test2_log_labels
# now we check for the october row...
which(tele_test2_log$month == "oct", arr.ind = T)
# we now know it's row 1723. So, let's remove that row.
tele_test2_log <- tele_test2_log[-which(tele_test2_log$month == "oct", arr.ind = T),]
# and now we "recreate" the labels.
tele_test2_log_labels <- tele_test2_log$y
tele_test2_log$y <- NULL

# and all good! Let's create a crosstable now.

improvedbinarypredictionlogt2 <- ifelse(predict(improvedpredictionmodelt2, newdata = tele_test2_log, type = "response") < 0.1, 0, 1)
CrossTable(tele_test2_log_labels, improvedbinarypredictionlogt2, prop.chisq = FALSE)
tele_test2_log_labels <- ifelse(tele_test2_log_labels == "yes", 1, 0)
confusionMatrix(data = as.factor(improvedbinarypredictionlogt2), reference = as.factor(tele_test2_log_labels), positive = "1")

# Cluster 4
# This cluster is somewhat straightforward; let's simply run the Crosstable.

improvedbinarypredictionlogt4 <- ifelse(predict(improvedpredictionmodelt4, newdata = tele_test4_log, type = "response") < 0.12, 0, 1)
CrossTable(tele_test4_log_labels, improvedbinarypredictionlogt4, prop.chisq = FALSE)
tele_test4_log_labels <- ifelse(tele_test4_log_labels == "yes", 1, 0)
confusionMatrix(data = as.factor(improvedbinarypredictionlogt4), reference = as.factor(tele_test4_log_labels), positive = "1")

# Cluster 5
# Similarly, we can simply run the Crosstable here.
improvedbinarypredictionlogt5 <- ifelse(predict(improvedpredictionmodelt5, newdata = tele_test5_log, type = "response") < 0.5, 0, 1)
CrossTable(tele_test5_log_labels, improvedbinarypredictionlogt5, prop.chisq = FALSE)
tele_test5_log_labels <- ifelse(tele_test5_log_labels == "yes", 1, 0)
confusionMatrix(data = as.factor(improvedbinarypredictionlogt5), reference = as.factor(tele_test5_log_labels), positive = "1")
```

## Interpreting our Logistic Regression 

### Cluster 2

Based on our model, we would end up calling 4 people in total, which gives total costs of €4. However, revenue is €6 with one call being successful. In this way, one would end up with a profit of **€2**, which means it would make sense to call these people. Compared to calling all, savings would then tally €1948 + €2 = **€1950**. However, one should still note that 107 ended up saying yes, but our model predicted them to say no. In other words, there is a lost revenue of **€642**. However, to get this number down, one would also have to call more people saying no, bringing up the costs.

### Cluster 4

Cluster 4 yields the same profit as cluster 2. Based on the model, 4 people should be called, essentially meaning costs of **€4**. Yet, with one call being successful, total profit is 1 * €6 - €4 = **€2**. Including the incremental savings from not calling anyone, this would result in incremental profits of €1238 + €2 = **€1240**.
In this way, the logistic regression model for cluster 4 is also profitable. Yet, one should still notice the missing revenue from people that would not be called (based on the model) - this number is equal to €56 * €6 = **€336**.

### Cluster 5

The logistic regression model for cluster 5 essentially tells us that no one should be called -- it only predicts “0”. Thus, the incremental profits relative to calling everyone is simply equal to the number of people not called - **€1855.** In this way, there will be no costs and no revenue. 82 people in this cluster turned out to be buyers, so there will be a lost revenue of €82 * €6 = **€492**.

### Final Conclusion on LR

LR results in total incremental profits, excluding lost sales, of 1950 + 1240 + 1855 = **€5045**.

# ANN
## Preparing datasets for modeling
```{r}
# Create dataframes with the three clusters using conditional filtering
library(tidyverse)
normcluster2 <- tele_norm_cluster[tele_norm_cluster$cluster == 2,] %>% select(-cluster)
normcluster4 <- tele_norm_cluster[tele_norm_cluster$cluster == 4,] %>% select(-cluster)
normcluster5 <- tele_norm_cluster[tele_norm_cluster$cluster == 5,] %>% select(-cluster)
```

## Let's create train and test datasets for each cluster
```{r}
# Do an approximate 80-20 train-test split. We prevented hard-coding this by using the floor() function.

tele_train2 <- normcluster2[1:floor(0.8*nrow(normcluster2)),]
tele_test2 <- normcluster2[(floor(0.8*nrow(normcluster2)+1)):nrow(normcluster2),]

tele_train4 <- normcluster4[1:floor(0.8*nrow(normcluster4)),]
tele_test4 <- normcluster4[(floor(0.8*nrow(normcluster4)+1)):nrow(normcluster4),]

tele_train5 <- normcluster5[1:floor(0.8*nrow(normcluster5)),]
tele_test5 <- normcluster5[(floor(0.8*nrow(normcluster5)+1)):nrow(normcluster5),]
```

## Building our initial model

```{r, cache = TRUE}
library(neuralnet)
tele_model2 <- neuralnet(y ~., data = tele_train2, hidden = 1)
plot(tele_model2)

tele_model4 <- neuralnet(y ~ ., data = tele_train4, hidden = 1, stepmax = 1e+08)
plot(tele_model4)

tele_model5 <- neuralnet(y ~ ., data = tele_train5, hidden = 1, stepmax = 1e+08)
plot(tele_model5)

# What we see already from the plot is that every feature has one input node, that there is one hidden layer with a bias term and one output node with a bias term (you won't be able to see this for some reason... - just take our word for it!)
```

## Initial Results from our ANN Model
```{r}
# First, we must load the neuralnet package.

library(neuralnet)

# Before we do anything, we have to remove the row with october as month in our ANN model, too, to allow for fair comparisons. This is done by using conditional filtering.

tele_test2 <- tele_test2[-which(tele_test2$monthoct == 1, arr.ind = T),]

# Let's exclude the y column in our tele_test. We don't want to run ANN with that included.
modelresults2 <- compute(tele_model2, tele_test2[1:53])
modelresults4 <- compute(tele_model4, tele_test4[1:53])
modelresults5 <- compute(tele_model5, tele_test5[1:53])

predictedtele2 <- modelresults2$net.result
summary(predictedtele2)
# We already see that the values are incredibly low. This is because the success rates of this cluster are incredibly low.
# Note that this isn't bad, per se - it just tells us that we shouldn't call this cluster.
predictedtele4 <- modelresults4$net.result
summary(predictedtele4)
# See interpretation for predictedtele2 - the same applies here.
predictedtele5 <- modelresults5$net.result
summary(predictedtele5)
# This seems to be all correct thus far... now, let's actually create a binary value.

annprediction2 <- ifelse(predictedtele2 < 0.5, 0, 1)
annprediction4 <- ifelse(predictedtele4 < 0.5, 0, 1)
annprediction5 <- ifelse(predictedtele5 < 0.5, 0, 1)

# Now, we must make a Cross Table and Confusion Matrix.

library(gmodels)
library(caret)

# For cluster = 2
CrossTable(tele_test2$y, annprediction2, prop.chisq = FALSE)
confusionMatrix(data = as.factor(annprediction2), reference = as.factor(tele_test2$y), positive = "1")

# For cluster = 4
CrossTable(tele_test4$y, annprediction4, prop.chisq = FALSE)
confusionMatrix(data = as.factor(annprediction4), reference = as.factor(tele_test4$y), positive = "1")

# For cluster = 5
CrossTable(tele_test5$y, annprediction5, prop.chisq = FALSE)
confusionMatrix(data = as.factor(annprediction5), reference = as.factor(tele_test5$y), positive = "1")
```

## Interpreting Our Initial ANN

This interpretation section - and any following interpretation sections - have been separated **per cluster**. As a disclaimer, *we did try to tinker with the cut-off value of 0.5, but this either (1) only made the model less accurate because the trained model is mainly based on unsuccessful calls, or (2) increased the accuracy only marginally. In the latter case, we carried this forward but since this already counts towards "improving the model", we'll only incorporate this later on*. Instead, we'll try to improve the model by adding hidden layers later on. 

### Cluster 2

With a cut-off point at 0.5, our ANN model never predicts that someone will say yes. Although this may seem weird at first, the model is trained with over 94% of people actually saying no, so it does not learn that well what differentiates a yes from a no. That said, it predicted that *all 1952 people would say no*, being correct 94.47% of the time. 
Excluding lost sales, this infers that we wouldn't call anyone and thus break-even at *€0*.

Including lost sales infers that we have to include the false negatives, which are worrying. We could have gotten 108 "yes'" if our ANN model were a bit stronger, resulting in lost sales of 108 * 6 = *€648*. That said, with such a model, we would prevent calling 1845 people that wouldn't have given us the yes-word anyways [kappa = 0 since detection rate = 0]. This proves that such a strategy significantly outperforms the idea of calling every single person, despite the lost sales.

### Cluster 4

With a cut-off point at 0.5, our ANN model predicts that someone will say no 1234 times, whilst only being correct 1180 times (95.6%), whereas it predicts a yes 8 times, whilst only being correct 3 times (37.5%).
Excluding lost sales, this infers that we would make profits of (6-1) * 3 - 5 * 1 = **€10**.

Including lost sales, however, infers that we must also include the 54 false negatives, resulting in net of €10 - 54 * 6 = - €314, so a net negative of **€314**. Again, however, such a model again shows that we shouldn't call everyone [kappa = 0.08].

### Cluster 5

With a cut-off point at 0.5, our ANN model predicts that someone will say no 1854 times, whilst only being correct 1773 times (95.6%), whereas it predicts a yes 1 time, which is a correct prediction. Excluding lost sales, that means that we'd make profits of 1 * (6-1) = **€5.**. Including the savings from NOT calling the people we previously would, our incremental profits are €5 + €1854 = **€1859**.

That said, when we include lost sales, we better understand the impact of 81 false negatives, resulting in a net of 5 - 6 * 81 = -€481, so a net negative of **€481**. Thankfully, we wouldn't incur unnecessary costs by calling someone who wouldn't say yes anyways! In other words, all our calls are worth it! Again, however, such a model again shows that we shouldn't call everyone [kappa = 0.02].

## Improving Our ANN Model

```{r}
# Let's do everything again, but now adding some additional layers to the ANN cluster models.

# Choosing the hidden levels is somewhat arbitrary. To not overload our computers, we went for hidden = 2.

improvedtele_model2 <- neuralnet(y ~., data = tele_train2, hidden = 2)
plot(tele_model2)

improvedtele_model4 <- neuralnet(y ~ ., data = tele_train4, hidden = 2, stepmax = 1e+08)
plot(tele_model4)

improvedtele_model5 <- neuralnet(y ~ ., data = tele_train5, hidden = 2, stepmax = 1e+08)
plot(tele_model5)

```

## Results from our Improved ANN Model

```{r}
# Let's exclude the y column in our tele_test. We don't want to run ANN with that included.
improvedmodelresults2 <- compute(improvedtele_model2, tele_test2[1:53])
improvedmodelresults4 <- compute(improvedtele_model4, tele_test4[1:53])
improvedmodelresults5 <- compute(improvedtele_model5, tele_test5[1:53])

improvedpredictedtele2 <- improvedmodelresults2$net.result
summary(improvedpredictedtele2)
# We already see that the values stay incredibly low. Again, this is because the success rates of this cluster are incredibly low.
# Note, again, that this isn't bad, per se - it just tells us that we shouldn't call this cluster.
improvedpredictedtele4 <- improvedmodelresults4$net.result
summary(improvedpredictedtele4)
# See interpretation for predictedtele2 - the same applies here.
improvedpredictedtele5 <- improvedmodelresults5$net.result
summary(improvedpredictedtele5)
# This seems to be all correct thus far... now, let's actually create a binary value.

improvedannprediction2 <- ifelse(improvedpredictedtele2 < 0.5, 0, 1)
improvedannprediction4 <- ifelse(improvedpredictedtele4 < 0.22, 0, 1)
improvedannprediction5 <- ifelse(improvedpredictedtele5 < 0.5, 0, 1)

# Now, let's create a Cross Table and Confusion Matrix.

# For cluster = 2
CrossTable(tele_test2$y, improvedannprediction2, prop.chisq = FALSE)
confusionMatrix(data = as.factor(improvedannprediction2), reference = as.factor(tele_test2$y), positive = "1")

# For cluster = 4
CrossTable(tele_test4$y, improvedannprediction4, prop.chisq = FALSE)
confusionMatrix(data = as.factor(improvedannprediction4), reference = as.factor(tele_test4$y), positive = "1")

# For cluster = 5
CrossTable(tele_test5$y, improvedannprediction5, prop.chisq = FALSE)
confusionMatrix(data = as.factor(improvedannprediction5), reference = as.factor(tele_test5$y), positive = "1")

```

## Interpreting our Improved ANN Model

> For this section, we *DID* adjust the cut-off points using a trial-and-error method to ensure that the accuracy estimates are as high as possible. As a rule of thumb, we tried to prevent as many false negatives as possible whilst preventing that false positives would grow significantly (line of thought: $6 in lost sales > $1 in costs, but not worth it if one decrease in FN comes paired with 6 or more increases in FP).

### Cluster 2

For cluster 2, it's still best to have a cut-off point of 0.5, purely because it's simply not worth it to call anyone in this segment given the ANN model. Lowering the cut-off point would result in a lot of costs from calling people who wouldn't say yes to our offering. This infers that still, excluding lost sales, we'd break even at **€0**. Compared to the old model, we would then be saving **€1952**.  Meanwhile, with lost sales, we'd incur losses of 108 * - 6 = **-€648**. That said, if we were to call everyone, we'd have profits of 108 * 6 - 1952 * 1 = **-€1304**. So, our incremental profits would be 1305 - 648 = **€657** (although we'd still make losses due to lost sales). 
### Cluster 4

For cluster 4, it's best to have a cut-off point of 0.22, because our ANN has a very low prediction value given the low success rate of the fourth cluster AND because reducing false negatives (-6) is more important than lowering false positives (-1). Excluding lost sales, the profits would now be 3 * (6-1) - 3 = **€12**. Because we don't call 1236 people, too, total incremental profits compared to the old model will be 1236 * 1 + 12 = **€1248**. When we include lost sales, though, we'd incur a net negative of 54 * €6 - €12 = **€312**, so unfortunately still a loss. Incremental profits, however, would be positive. If we were to call everyone, we'd have profits of 57 * 6 - 1242 * 1 = **€900**, making incremental profits of 900 - 312 = **€588** by implementing an ANN model.

### Cluster 5

For cluster 5, we see something very interesting. We denote that *regardless of the cut-off point, the ANN model with two hidden layers will have lower profits than with an ANN with one hidden layer*. As such, conclusions for this improved model are equal to the conclusions for the initial model. Thus, we should overwrite the improved model. Savings stay at **€1859**.

### Final Conclusion on ANN

ANN still works well - savings stay at 1952 + 1248 + 1859 = **€5059**.

### Overwriting the improved model

```{r}
improvedannprediction5 <- annprediction5
```

### Final Conclusion

> Simply put, ANN works. We cannot make these low-success clusters into money machines, and the model is not perfect in preventing us from calling anyone who will say no, but we have saved costs tremendously by using ANN. Savings are 657 + 591 + 875 = €2123.

# KNN

```{r}
library(class)
# We know that for KNN, we have to use a train and test set too, but here we must exclude the response variable.

# Before we do anything, let's remove the october record in our test set; this is already done since we removed it from the tele_test2$y column to begin with,

# So, all we do is create two label variables instead and drop the response variable from the train and test set.

tele_test2knn_labels <- tele_test2$y
tele_test4knn_labels <- tele_test4$y
tele_test5knn_labels <- tele_test5$y

tele_train2_labels <- tele_train2$y
tele_train4_labels <- tele_train4$y
tele_train5_labels <- tele_train5$y

# Let's now set k2
k2 <- sqrt(nrow(tele_train2))
# So let's use k2 = 89
k2 <- 89
# Let's now set k4
k4 <- sqrt(nrow(tele_train4))
# So let's use k4 = 71
k4 <- 71
# Let's  now set k5
k5 <-sqrt(nrow(tele_train5))
# So let's use k5 = 87
k5 <- 87

tele_test2_pred_knn <- knn(train = tele_train2, test = tele_test2, cl = tele_train2_labels, k = k2)

CrossTable(as.factor(tele_test2knn_labels), tele_test2_pred_knn, prop.chisq = F)
confusionMatrix(data = tele_test2_pred_knn, reference = as.factor(tele_test2knn_labels), positive = "1")

tele_test4_pred_knn <- knn(train = tele_train4, test = tele_test4, cl = tele_train4_labels, k = k4)

CrossTable(as.factor(tele_test4knn_labels), tele_test4_pred_knn, prop.chisq = F)
confusionMatrix(data = tele_test4_pred_knn, reference = as.factor(tele_test4knn_labels), positive = "1")

tele_test5_pred_knn <- knn(train = tele_train5, test = tele_test5, cl = tele_train5_labels, k = k5)

CrossTable(as.factor(tele_test5knn_labels), tele_test5_pred_knn, prop.chisq = F)
confusionMatrix(data = tele_test5_pred_knn, reference = as.factor(tele_test5knn_labels), positive = "1")
```

## Interpreting the kNN

### Cluster 2

Our kNN model never predicts that someone will say yes. Although this may seem weird at first, the model is trained with over *94%* of people actually saying no, so it does not learn that well what differentiates a yes from a no. That said, it predicted that all 1952 people would say no, being correct 94.47% of the time. The false negatives are worrying, though. We could have gotten 108 "yes'" if our kNN model were a bit stronger. That said, with such a model, we would prevent calling 1845 people that wouldn't have given us the yes-word anyways [kappa = 0 since detection rate = 0]. This proves that such a strategy significantly outperforms the idea of calling every single person. In total, excluding the loss of potential sales, the model saved the company **€1952** by not calling anyone in the cluster.

Using the kNN model on this cluster would result in us not calling 108 "yes" customers which would result in a loss of a potential €648 , however our overall costs would be less than if we were to call everyone 108 * 6 - 1952 * 1 = -€1304. So, our incremental profits would be 1304 - 648 = **€656**. However this also raises the possibility of calling more people and inducing more costs.

### Cluster 4

As with the previous cluster, our kNN model never predicts that someone will say yes for the same reasons stated previously. The model predicts that every single customer in the cluster would say no, doing it with a **95.41%** accuracy. This cluster has the same issue with false negatives as cluster 2. In total, excluding the loss of potential sales, the model saved the company **€1242** by not calling anyone in the cluster.

Including the lost earnings from our false negatives, the company would save out on spending 57 * 6 - 1242 = €900 on calls that would never return a profit. Thus, using the kNN model saves the company 900 - 342 = **€558**.
 

### Cluster 5

Using the kNN model on cluster 5 leaves us again with no one saying yes. The model once again predicts that everyone will say no. With an accuracy of **95.58%** the model once again leaves us with a set of false negatives, this time 82, meaning that the company will lose out on 82 * 6 = €492 in earnings. In total, excluding the loss of potential sales, the model saved the company **€1855** by not calling anyone in the cluster. 

Including the lost earnings from our false negatives, the company would save out on spending 82 * 6 - 1773 = €1281 on calls that would never return a profit. Thus, using the kNN model saves the company 1281 - 342 = **€789**.

### Final Conclusion on kNN
kNN works to some extent. Using the model we did save the company thousands in reduced costs, because we would never have called a single customer in the respective clusters. The total amount saved across the three clusters were **5049**. For another project there could be value in searching through different k-values to be used for the different clusters.

# A Combined Prediction Model

To start building a combined prediction model, we need a new dataframe with all output values (0,1). 
```{r}

combinedoutputs2 <- data.frame(improvedannprediction2, improvedbinarypredictionlogt2, tele_test2_pred_knn)
colnames(combinedoutputs2) <- c("ANN", "LogReg", "kNN")

combinedoutputs4 <- data.frame(improvedannprediction4, improvedbinarypredictionlogt4, tele_test4_pred_knn)
colnames(combinedoutputs4) <- c("ANN", "LogReg", "kNN")

combinedoutputs5 <- data.frame(improvedannprediction5, improvedbinarypredictionlogt5, tele_test5_pred_knn)
colnames(combinedoutputs5) <- c("ANN", "LogReg", "kNN")
str(combinedoutputs5)
# We see that our kNN column is a factor. Let's change that into a numeric for our ifelse later on.

combinedoutputs2$kNN <- ifelse(combinedoutputs2$kNN == "0", 0, 1)
combinedoutputs4$kNN <- ifelse(combinedoutputs4$kNN == "0", 0, 1)
combinedoutputs5$kNN <- ifelse(combinedoutputs5$kNN == "0", 0, 1)

# Let's have a look again...
str(combinedoutputs2)
str(combinedoutputs4)
str(combinedoutputs5)

# Everything's all good! Let's now have a real combined output.
combinedoutputs2$mix <- ifelse(combinedoutputs2$ANN + 
                                combinedoutputs2$LogReg + 
                                combinedoutputs2$kNN > 1, 1, 0)

combinedoutputs4$mix <- ifelse(combinedoutputs4$ANN + 
                                combinedoutputs4$LogReg + 
                                combinedoutputs4$kNN > 1, 1, 0)

combinedoutputs5$mix <- ifelse(combinedoutputs5$ANN + 
                                combinedoutputs5$LogReg + 
                                combinedoutputs5$kNN > 1, 1, 0)

# Great! Now let's make a Cross Table and Confusion matrix again.

# Store mix results into a numeric.
combinedresults2 <- as.numeric(combinedoutputs2$mix)
combinedresults4 <- as.numeric(combinedoutputs4$mix)
combinedresults5 <- as.numeric(combinedoutputs5$mix)

# Create a cross table
# Note that, for convenience, we can 
CrossTable(tele_test2knn_labels, combinedresults2, prop.chisq = F)
CrossTable(tele_test4knn_labels, combinedresults4, prop.chisq = F)
CrossTable(tele_test5knn_labels, combinedresults5, prop.chisq = F)

# Create a confusion matrix
confusionMatrix(data = as.factor(combinedresults2), reference = as.factor(tele_test2knn_labels), positive = "1")

confusionMatrix(data = as.factor(combinedresults4), reference = as.factor(tele_test4knn_labels), positive = "1")

confusionMatrix(data = as.factor(combinedresults5), reference = as.factor(tele_test5knn_labels), positive = "1")
```

## Improving our Combined Prediction Model - An Interesting Task

The way to improve a combined model is by improving its roots (being the ANN, LR, and kNN). That said, if we were allowed to add weights to the sum, things would be interesting. We'd add a higher weight to the most accurate model and similarly penalize less accurate models. However, as the model was described in class as the sum of the LR, KNN, and ANN output, we decided *not to tinker with weights.*

## Interpreting our Combined Prediction Model

### Cluster 2

When looking at the Combined Cluster 2 model, we see that it would be unprofitable for the firm to call anyone as it predicts that 1185 people would not buy, while only 57 people would. Therefore, we recommend not using this combined model for prediction as it would result in a net loss of: (108 * 5) -(1844 * 1) = **-€1304**.
Instead, it would be better for the firm to not call anyone in cluster 2, which would end up with a net value of **€0**. We acknowledge that the firm is leaving potential sales of 108 * 6 = **€648** on the table by not calling anyone in this cluster but is outweighed by the high costs of calling the 1844 other people as the model predicts. That said, the savings by not calling anyone in cluster 2 is **€1952**.

### Cluster 4

When looking at the Combined Cluster 4 model, we see that it is unprofitable for the firm to call anyone as it predicts that only 57 people would buy out of the total 1242 in this cluster. This means that the firm shouldn’t call anyone in this Cluster as the combined model predicts a net loss of: (57 * 5) - (1185 * 1) = *-€900* Instead, they should drop this Cluster which would result in a net profit/loss of **€0**. By not calling Cluster 4, the company would leave a revenue of  57 * 6 = **€342**, but since we can’t surgically call the 57 people who are predicted to buy in this cluster, we would end up with a net loss as predicted by the model. That said, the savings by not calling anyone in cluster 4 is **€1242**.

### Cluster 5

When looking at the Combined Cluster 5 model, we see that it would also not be profitable as it predicts that only 82 out of the 1855 persons in this cluster would by. Calling everyone in this cluster would result in a net loss of: (82 * 5) - (1773 * 1) = *-€1363*. By not calling anyone in this cluster the firm would not incur any costs or sales with this cluster which would result in a net profit of 0. They are, however, leaving sales on the table by not calling anyone in this cluster equal to (82 * 6) = *€492*. That said, the savings by not calling anyone in cluster 5 is **€1855**.

As such, the savings in total are **€5049**.

# Conclusion and Recommendations

## Profits without any model

> The reasoning we will use in this section is as follows: we assume that we still call everyone in the clusters 1 and 3. Next, we will call everyone in our train sets. Yet, we will change the people whom we call in our test sets based on the models we ran above. That said, we can already state that the final result will be negative! Yet, from a managerial perspective, we merely care about the impact of running such a model (i.e., what are the actual savings made). As such, we will only consider the impact on the test sets (calling everyone vs. model output).

## Profits from calling everyone in test set 2,4, and 5

```{r}
# Formula: revenue for all yes' and €1 for everyone we call (being everyone in our test set)
profitsts2 <- sum(tele_test2$y) * 6 - nrow(tele_test2)
profitsts4 <- sum(tele_test4$y) * 6 - nrow(tele_test4)
profitsts5 <- sum(tele_test5$y) * 6 - nrow(tele_test5)
profitsts245 <- profitsts2 + profitsts4 + profitsts5
```

## Incremental Savings with LR

Without a model, we'd lose €3567. The model suggests that we call some people, resulting in savings of €5045 (see LR section), so incremental profits of -3567 + 5045 = **€1478**. 

## Incremental Savings with ANN

Without a model, we'd lose €3567. The model suggests that we call some people, resulting in savings of €5059 (see ANN section), so incremental profits of -3567 + 5059 = **€1492**.

## Incremental Savings with kNN

Without a model, we'd lose €3567. The model suggests that we don't call anyone, resulting in savings of €5049 (see KNN section), so incremental profits of -3567 + 5049 = **€1482**.

## Incremental Savings with Combined Model

Without a model, we'd lose €3567. The model suggests that we don't call anyone, resulting in savings of €5049 (see Combined Prediction Model section), so incremental profits of -3567 + 5049 = **€1482**.
 

# FINAL CONCLUSION - BEST MODEL

> The best model is the ANN model, where the ANN model for cluster 2 and 4 house 2 layers each, and cluster 5 houses 1 layer (resulting in savings of €1492. Regardless, all models seem to be eeringly similar due to the skewed nature of the clustered data - 2,4, and 5 all indicate that almost no one will actually buy the firm's offering. Although this may appear as a somewhat negative outcome at first, it is vital to understand that whenever the telemarketing firm is able to categorize the next potential client in one of the clusters, it can save money by simply not calling the person to begin with. That said, the models - and especially the ANN model - can help us predict that there might be someone in the clusters who will answer the phone if we were to call them. Anew, however, we must mention that all models point us in a similar direction, which is calling almost no one, or no one at all. With that, there is some facet of indifference in our conclusion, although the data points towards a slight preference for the ANN model.  

